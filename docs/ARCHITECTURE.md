Architecture and Theoretical Foundations

This project implements a discrete-time research sandbox for optimal market making and order flow modeling. While the simulator operates in discrete time, its conceptual foundation is rooted in continuous-time stochastic control.

Connection to stochastic control
The canonical formulation of optimal market making is given by the Avellaneda–Stoikov framework, which models the problem as a stochastic control problem with inventory-dependent risk. The optimal quotes arise as the solution of a Hamilton–Jacobi–Bellman (HJB) equation under exponential utility.

In continuous time, the control variable is the bid and ask quote distance, and the state variables include inventory and mid-price. The arrival intensities of market orders depend parametrically on the quote distance, leading to a non-linear HJB equation.

Discrete-time approximation
This project implements a discrete-time approximation of the continuous-time control problem. Quote decisions are updated on a fixed time grid, and order arrivals are simulated using Poisson processes with parametric intensities. While the exact HJB is not solved numerically, its structure motivates the quoting policies implemented in the strategy module.

Implemented policies
The Avellaneda–Stoikov policy is implemented as a benchmark strategy derived from the continuous-time solution under simplifying assumptions. Additional strategies introduce controlled deviations from the optimal quotes in order to study learning and identifiability effects.

Learning versus control
A key focus of this project is the interaction between control and learning. While the HJB framework assumes known model parameters, real-world implementations require calibration from data generated by the strategy itself. This feedback loop motivates the introduction of probing policies, which explicitly trade off optimal control and parameter identifiability.

Scope
The architecture is intentionally modular to allow extensions toward explicit numerical HJB solvers, reinforcement learning approaches, or multi-dimensional state spaces.